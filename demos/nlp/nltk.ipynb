{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f392373",
   "metadata": {},
   "source": [
    "# NLP using NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a8a9268",
   "metadata": {},
   "source": [
    "##### NLP Pipeline\n",
    "- Data collection\n",
    "- Tokenization, Stopword Removal, Stemming\n",
    "- Building a common vocab\n",
    "- Vectorize documents\n",
    "- Perform Classification/Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ba3c8f8",
   "metadata": {},
   "source": [
    "### 1) Data Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "295895cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     C:\\Users\\jomin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\jomin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\jomin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('brown')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "acd82674",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import brown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "91b56a52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['adventure', 'belles_lettres', 'editorial', 'fiction', 'government', 'hobbies', 'humor', 'learned', 'lore', 'mystery', 'news', 'religion', 'reviews', 'romance', 'science_fiction']\n"
     ]
    }
   ],
   "source": [
    "print(brown.categories())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bbda087b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'nltk.collections.LazySubsequence'> 100\n",
      "[['Assembly', 'session', 'brought', 'much', 'good'], ['The', 'General', 'Assembly', ',', 'which', 'adjourns', 'today', ',', 'has', 'performed', 'in', 'an', 'atmosphere', 'of', 'crisis', 'and', 'struggle', 'from', 'the', 'day', 'it', 'convened', '.'], ...]\n"
     ]
    }
   ],
   "source": [
    "data = brown.sents(categories='editorial')[:100]\n",
    "print(type(data), len(data))\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b66aa5d5",
   "metadata": {},
   "source": [
    "### 2.1) Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "95ef201f",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'It was a very pleasant day, the weather was cool and there were showers. I went to the market to buy some fruits.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0547fb5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "34678f10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['It was a very pleasant day, the weather was cool and there were showers.',\n",
       " 'I went to the market to buy some fruits.']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sents = sent_tokenize(text)\n",
    "sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "164e68c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['it', 'was', 'a', 'very', 'pleasant', 'day', ',', 'the', 'weather', 'was', 'cool', 'and', 'there', 'were', 'showers', '.']\n"
     ]
    }
   ],
   "source": [
    "word_list = word_tokenize(sents[0].lower())\n",
    "print(word_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26141693",
   "metadata": {},
   "source": [
    "### 2.2) Stopword Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e179073c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d7816de2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'themselves', 'don', \"they're\", \"they've\", 'aren', 'their', \"we've\", 'theirs', 'been', 'to', 'other', \"wouldn't\", \"couldn't\", 'be', 'had', 'now', 'mightn', 'shan', 'so', \"he'll\", 'll', 'are', 'whom', 'needn', 'up', 'didn', 'him', \"hasn't\", 'too', 'being', 'm', 'myself', 'his', 'an', 'weren', 'd', 'itself', \"shan't\", 'which', 'just', \"you're\", 'her', \"they'd\", 'do', 'into', 'some', 'have', 'hadn', 'yourselves', 'can', \"needn't\", 'under', 'during', \"we'd\", 'how', 'who', 'only', 'until', 'of', 'more', 'those', 'with', 'you', \"it's\", \"hadn't\", \"it'll\", 'its', \"she's\", 'were', 'further', 'herself', 're', 't', 'by', \"they'll\", 'did', 'should', 'most', 'same', 'then', 'hers', 'doesn', 'o', \"should've\", 'if', 'himself', 'isn', \"doesn't\", 'both', \"shouldn't\", 'about', \"wasn't\", \"i'll\", \"isn't\", \"it'd\", 'after', 'me', 'won', 'because', 'there', 'hasn', \"that'll\", 'and', 'each', 'it', 'off', 'this', \"weren't\", 'not', 'will', 'no', \"don't\", 'am', 'in', 'we', 'before', 'has', 'once', 'wasn', 'our', 'against', 'your', 'as', 'any', 'own', 'or', \"you'll\", 'at', 'she', 'here', 'my', 'through', \"i'd\", \"won't\", \"i'm\", 'was', 'wouldn', 'but', 'very', 'he', 'below', 'is', 'they', \"you've\", 'why', \"you'd\", \"we'll\", 'couldn', \"he's\", 'for', 'than', \"aren't\", 'mustn', \"she'll\", 'between', 'shouldn', 'ma', 'nor', 'having', \"haven't\", 'these', 'few', 'yours', 'above', \"he'd\", 'a', 'i', \"mustn't\", \"didn't\", 'does', \"mightn't\", 'the', 'yourself', 'where', 'from', 've', 'haven', \"she'd\", 'them', 'that', 's', 'doing', 'out', 'y', 'over', 'ourselves', \"we're\", 'when', 'such', 'ours', 'what', 'while', 'ain', 'all', 'on', \"i've\", 'down', 'again'}\n",
      "198\n"
     ]
    }
   ],
   "source": [
    "sw = set(stopwords.words('english'))\n",
    "print(sw)\n",
    "print(len(sw))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa0c275f",
   "metadata": {},
   "source": [
    "#### Filter words from sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "446de0d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_words(word_list):\n",
    "    useful_words = [w for w in word_list if w not in sw]\n",
    "    return useful_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "70ff95d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['pleasant', 'day', ',', 'weather', 'cool', 'showers', '.']\n"
     ]
    }
   ],
   "source": [
    "useful_words = filter_words(word_list)\n",
    "print(useful_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f04155e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "04a25edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer('[a-zA-Z0-9]+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f57c9544",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['send', 'the', '50', 'documents', 'to', 'abc', 'def', 'ghi']\n"
     ]
    }
   ],
   "source": [
    "sent = 'send the 50 documents to abc, def, ghi.'\n",
    "print(tokenizer.tokenize(sent))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8469e553",
   "metadata": {},
   "source": [
    "### 2.3) Stemming\n",
    "- Process that transforms particular words into root words\n",
    "- Jumping, jump, jumps, jumped => jump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "34bdbbbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'The quick brown fox was seen jumping on the lazy dog from a high wall. Foxes love to make jumps.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "808a9357",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the',\n",
       " 'quick',\n",
       " 'brown',\n",
       " 'fox',\n",
       " 'was',\n",
       " 'seen',\n",
       " 'jumping',\n",
       " 'on',\n",
       " 'the',\n",
       " 'lazy',\n",
       " 'dog',\n",
       " 'from',\n",
       " 'a',\n",
       " 'high',\n",
       " 'wall',\n",
       " 'foxes',\n",
       " 'love',\n",
       " 'to',\n",
       " 'make',\n",
       " 'jumps']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_list = tokenizer.tokenize(text.lower())\n",
    "word_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0489df9",
   "metadata": {},
   "source": [
    "### Types of Stemmers\n",
    "- Snowball Stemmer (Multilingual)\n",
    "- Porter Stemmer (English only)\n",
    "- Lancaster Stemmer (English only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e108075c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import PorterStemmer, SnowballStemmer\n",
    "from nltk.stem.lancaster import LancasterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b47a67ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f0cf45a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'jump'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ps.stem('jumped')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a9ba576c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'jump'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ps.stem('jumping')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8aeb3de5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'love'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ps.stem('lovely')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "18c77438",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'awesom'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ps.stem('awesome')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d6ef8c6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'awesom'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ls = LancasterStemmer()\n",
    "ls.stem('awesome')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cc315dcd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'teen'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ls.stem('teenager')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "53a69246",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'teenag'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ps.stem('teenager')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5cf4ec13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'love'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ss = SnowballStemmer('english')\n",
    "ss.stem('lovely')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "de34f869",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cour'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ss = SnowballStemmer('french')\n",
    "ss.stem('courais')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c0a1915",
   "metadata": {},
   "source": [
    "#### Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9545b16c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Indian cricket team will win world cup, says Indian captain Virat Kohli. World Cup will be held in India during the next year', 'We will win next Lok Sabha election, says Indian PM', 'The noble Rabindranath Tagore won the hearts of the people', 'The movie Raazi has an exciting trailer. It is based on a real story']\n"
     ]
    }
   ],
   "source": [
    "# corpus = collection of documents\n",
    "corpus = [\n",
    "    'Indian cricket team will win world cup, says Indian captain Virat Kohli. World Cup will be held in India during the next year',\n",
    "    'We will win next Lok Sabha election, says Indian PM',\n",
    "    'The noble Rabindranath Tagore won the hearts of the people',\n",
    "    'The movie Raazi has an exciting trailer. It is based on a real story'\n",
    "]\n",
    "\n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "967d17cf",
   "metadata": {},
   "source": [
    "- Converting words into numerical features\n",
    "- Building a common vocabulary and vectorize the documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "15ba9c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def myTokenizer(sent):\n",
    "    words = tokenizer.tokenize(sent.lower())\n",
    "    return filter_words(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d1de3172",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['indian',\n",
       " 'cricket',\n",
       " 'team',\n",
       " 'win',\n",
       " 'world',\n",
       " 'cup',\n",
       " 'says',\n",
       " 'indian',\n",
       " 'captain',\n",
       " 'virat',\n",
       " 'kohli',\n",
       " 'world',\n",
       " 'cup',\n",
       " 'held',\n",
       " 'india',\n",
       " 'next',\n",
       " 'year']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myTokenizer(corpus[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ad97115f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ded0e212",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(tokenizer=myTokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7202c3e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jomin\\Documents\\Code\\CodingClub\\AI-Mafia-Machine-Learning\\.env-mafia\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "vectorized_corpus = cv.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "bc6d0df5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Compressed Sparse Row sparse matrix of dtype 'int64'\n",
      "\twith 34 stored elements and shape (4, 30)>\n",
      "  Coords\tValues\n",
      "  (0, 9)\t2\n",
      "  (0, 2)\t1\n",
      "  (0, 24)\t1\n",
      "  (0, 27)\t1\n",
      "  (0, 28)\t2\n",
      "  (0, 3)\t2\n",
      "  (0, 21)\t1\n",
      "  (0, 1)\t1\n",
      "  (0, 26)\t1\n",
      "  (0, 10)\t1\n",
      "  (0, 7)\t1\n",
      "  (0, 8)\t1\n",
      "  (0, 13)\t1\n",
      "  (0, 29)\t1\n",
      "  (1, 9)\t1\n",
      "  (1, 27)\t1\n",
      "  (1, 21)\t1\n",
      "  (1, 13)\t1\n",
      "  (1, 11)\t1\n",
      "  (1, 20)\t1\n",
      "  (1, 4)\t1\n",
      "  (1, 16)\t1\n",
      "  (2, 14)\t1\n",
      "  (2, 18)\t1\n",
      "  (2, 23)\t1\n",
      "  (2, 6)\t1\n",
      "  (2, 15)\t1\n",
      "  (3, 12)\t1\n",
      "  (3, 17)\t1\n",
      "  (3, 5)\t1\n",
      "  (3, 25)\t1\n",
      "  (3, 0)\t1\n",
      "  (3, 19)\t1\n",
      "  (3, 22)\t1\n"
     ]
    }
   ],
   "source": [
    "print(vectorized_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "57c07bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "vc = vectorized_corpus.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1c8fff68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 1 2 0 0 0 1 1 2 1 0 0 1 0 0 0 0 0 0 0 1 0 0 1 0 1 1 2 1]\n",
      "{'indian': 9, 'cricket': 2, 'team': 24, 'win': 27, 'world': 28, 'cup': 3, 'says': 21, 'captain': 1, 'virat': 26, 'kohli': 10, 'held': 7, 'india': 8, 'next': 13, 'year': 29, 'lok': 11, 'sabha': 20, 'election': 4, 'pm': 16, 'noble': 14, 'rabindranath': 18, 'tagore': 23, 'hearts': 6, 'people': 15, 'movie': 12, 'raazi': 17, 'exciting': 5, 'trailer': 25, 'based': 0, 'real': 19, 'story': 22}\n"
     ]
    }
   ],
   "source": [
    "print(vc[0])\n",
    "print(cv.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "514fff72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array(['captain', 'cricket', 'cup', 'held', 'india', 'indian', 'kohli',\n",
       "        'next', 'says', 'team', 'virat', 'win', 'world', 'year'],\n",
       "       dtype='<U12')]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.inverse_transform(vc[0].reshape(1,-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c92d2d",
   "metadata": {},
   "source": [
    "### TF IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "184bdf06",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c5a94695",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(tokenizer=myTokenizer, ngram_range=(1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0e191d82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.         0.         0.16227964 0.16227964 0.16227964 0.16227964\n",
      " 0.32455927 0.16227964 0.16227964 0.         0.         0.\n",
      " 0.         0.         0.         0.16227964 0.16227964 0.16227964\n",
      " 0.16227964 0.25588626 0.16227964 0.16227964 0.         0.16227964\n",
      " 0.16227964 0.         0.         0.         0.         0.12794313\n",
      " 0.         0.16227964 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.12794313 0.12794313 0.         0.\n",
      " 0.         0.16227964 0.16227964 0.         0.         0.16227964\n",
      " 0.16227964 0.12794313 0.         0.16227964 0.32455927 0.32455927\n",
      " 0.16227964]\n",
      "{'indian': 19, 'cricket': 4, 'team': 49, 'win': 55, 'world': 58, 'cup': 6, 'says': 44, 'captain': 2, 'virat': 53, 'kohli': 23, 'held': 15, 'india': 17, 'next': 29, 'year': 60, 'indian cricket': 21, 'cricket team': 5, 'team win': 50, 'win world': 57, 'world cup': 59, 'cup says': 8, 'says indian': 45, 'indian captain': 20, 'captain virat': 3, 'virat kohli': 54, 'kohli world': 24, 'cup held': 7, 'held india': 16, 'india next': 18, 'next year': 31, 'lok': 25, 'sabha': 42, 'election': 9, 'pm': 35, 'win next': 56, 'next lok': 30, 'lok sabha': 26, 'sabha election': 43, 'election says': 10, 'indian pm': 22, 'noble': 32, 'rabindranath': 38, 'tagore': 47, 'hearts': 13, 'people': 34, 'noble rabindranath': 33, 'rabindranath tagore': 39, 'tagore hearts': 48, 'hearts people': 14, 'movie': 27, 'raazi': 36, 'exciting': 11, 'trailer': 51, 'based': 0, 'real': 40, 'story': 46, 'movie raazi': 28, 'raazi exciting': 37, 'exciting trailer': 12, 'trailer based': 52, 'based real': 1, 'real story': 41}\n"
     ]
    }
   ],
   "source": [
    "vectorized_corpus = tfidf_vectorizer.fit_transform(corpus).toarray()\n",
    "print(vectorized_corpus[0])\n",
    "print(tfidf_vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d4c7dec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env-mafia",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
